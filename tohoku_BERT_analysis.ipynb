{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMk6li9bLw1YFNUF2AH9w4B",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ericodle/Semantic-Positioning-Model-for-Japanese-Adverbs/blob/main/tohoku_BERT_analysis.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Install required packages"
      ],
      "metadata": {
        "id": "25Qkl5qJybY8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers\n",
        "!pip install japanize_matplotlib"
      ],
      "metadata": {
        "id": "KQS5tJUfGmXC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Import script dependencies"
      ],
      "metadata": {
        "id": "ii1l_4ONydCb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import BertTokenizer, BertModel\n",
        "import torch\n",
        "import numpy as np\n",
        "from sklearn.cluster import KMeans\n",
        "from sklearn.decomposition import PCA\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import japanize_matplotlib\n",
        "from sklearn.metrics import silhouette_score\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from scipy.spatial.distance import cdist"
      ],
      "metadata": {
        "id": "p-QDaKccyhSj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Load model, load csv list of adverbs, and embed adverbs."
      ],
      "metadata": {
        "id": "kJPPErA5yjey"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the Tohoku Japanese BERT tokenizer and model. replace \"bert-large-japanese\" with \"bert-base-japanese\" as needed.\n",
        "tokenizer = BertTokenizer.from_pretrained(\"cl-tohoku/bert-large-japanese\")\n",
        "model = BertModel.from_pretrained(\"cl-tohoku/bert-large-japanese\")\n",
        "\n",
        "# Load the CSV file with adverbs\n",
        "csv_file_path = \"/content/unique_adverbs_column.csv\"\n",
        "\n",
        "# Read the CSV file using pandas\n",
        "data = pd.read_csv(csv_file_path, header=None, encoding=\"utf-8\")\n",
        "\n",
        "# Get the adverbs from the first column\n",
        "adverbs = data.iloc[:, 0].tolist()\n",
        "true_labels = data.iloc[:, 1].tolist()\n",
        "\n",
        "# Calculate embeddings and store them in a list\n",
        "embeddings = []\n",
        "for adverb in adverbs:\n",
        "    # Tokenize and convert to BERT input format\n",
        "    inputs = tokenizer(adverb, return_tensors=\"pt\")\n",
        "\n",
        "    # Get the embeddings from the BERT model\n",
        "    with torch.no_grad():\n",
        "        output = model(**inputs)\n",
        "        embedding = output.last_hidden_state[:, 0, :].numpy()  # Using [CLS] token embedding\n",
        "\n",
        "    embeddings.append(embedding)\n",
        "\n",
        "# Convert embeddings list to a numpy array\n",
        "embedding_array = np.array(embeddings)\n",
        "\n",
        "# Reshape the embedding array for PCA\n",
        "num_adverbs, num_tokens, embedding_dim = embedding_array.shape\n",
        "\n",
        "# Flatten the normalized embedding array before applying Min-Max Normalization\n",
        "flattened_embedding_array = embedding_array.reshape(-1, embedding_dim)"
      ],
      "metadata": {
        "id": "y8Qgr0JLGqpB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Normalize the embeding values"
      ],
      "metadata": {
        "id": "RYYSnR6xFu3D"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Apply Min-Max Normalization\n",
        "min_max_scaler = MinMaxScaler()\n",
        "min_max_normalized_embedding_array = min_max_scaler.fit_transform(flattened_embedding_array)\n",
        "\n",
        "# Reshape the normalized embedding array back to its original shape\n",
        "min_max_normalized_embedding_array = min_max_normalized_embedding_array.reshape(num_adverbs, num_tokens, embedding_dim)\n",
        "\n",
        "# Flatten the 3D array to a 2D array\n",
        "flattened_normalized_embedding_array = min_max_normalized_embedding_array.reshape(-1, embedding_dim)"
      ],
      "metadata": {
        "id": "i2do1JF71IBX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Reduce dimensionality by PCA, then cluster by K-means."
      ],
      "metadata": {
        "id": "huB3LMqFyoZW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Reduce dimensionality using PCA\n",
        "num_dimensions = 3  # Using 3 components for PCA\n",
        "pca = PCA(n_components=num_dimensions)\n",
        "reduced_embedding_array = pca.fit_transform(flattened_normalized_embedding_array)\n",
        "\n",
        "# Use K-Means clustering with 3 clusters\n",
        "num_clusters = 3\n",
        "kmeans = KMeans(n_clusters=num_clusters, random_state=0)\n",
        "cluster_labels = kmeans.fit_predict(reduced_embedding_array)\n",
        "\n",
        "# Generate a 3D scatter plot with cluster centroids\n",
        "fig = plt.figure(figsize=(10, 8))\n",
        "ax = fig.add_subplot(111, projection='3d')\n",
        "cluster_names = ['Cluster A', 'Cluster B', 'Cluster C']\n",
        "for i, cluster_name in enumerate(cluster_names):\n",
        "    ax.scatter(\n",
        "        reduced_embedding_array[cluster_labels == i, 0],\n",
        "        reduced_embedding_array[cluster_labels == i, 1],\n",
        "        reduced_embedding_array[cluster_labels == i, 2],  # Use the third component\n",
        "        label=cluster_name\n",
        "    )\n",
        "ax.scatter(\n",
        "    kmeans.cluster_centers_[:, 0],\n",
        "    kmeans.cluster_centers_[:, 1],\n",
        "    kmeans.cluster_centers_[:, 2],  # Use the third component\n",
        "    marker='X',\n",
        "    color='black',\n",
        "    label='Centroids'\n",
        ")\n",
        "\n",
        "# Label the points with adverbs and true labels\n",
        "for i, (adverb, true_label) in enumerate(zip(adverbs, true_labels)):\n",
        "    ax.text(\n",
        "        reduced_embedding_array[i, 0],\n",
        "        reduced_embedding_array[i, 1],\n",
        "        reduced_embedding_array[i, 2],  # Use the third component\n",
        "        f'{adverb} ({true_label})',\n",
        "        fontsize=8,\n",
        "        ha='center'\n",
        "    )\n",
        "\n",
        "ax.set_title('3D Cluster Plot with Centroids')\n",
        "ax.set_xlabel('Principal Component 1')\n",
        "ax.set_ylabel('Principal Component 2')\n",
        "ax.set_zlabel('Principal Component 3')\n",
        "ax.legend()\n",
        "\n",
        "# Save the plot as a PNG image with 600 DPI\n",
        "plt.savefig('pre_analysis_3d_scatterplot.png', dpi=600)\n",
        "\n",
        "# Display the plot\n",
        "plt.show()\n",
        "\n",
        "# Calculate the distance matrix between cluster centroids\n",
        "centroid_distances = cdist(kmeans.cluster_centers_, kmeans.cluster_centers_)\n",
        "\n",
        "# Print the distance matrix\n",
        "print(\"Distance Matrix of Centroids:\")\n",
        "print(centroid_distances)\n",
        "\n",
        "# Save the distance matrix to a CSV file\n",
        "np.savetxt(\"pre_dist.csv\", centroid_distances, delimiter=\",\")"
      ],
      "metadata": {
        "id": "14uYu0G-Gtmd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Perform Silhouette analysis"
      ],
      "metadata": {
        "id": "1w_tFevWF5L5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Silhouette Score\n",
        "sil_scores = []\n",
        "for n_clusters in range(2, 50):\n",
        "    kmeans = KMeans(n_clusters=n_clusters, random_state=0)\n",
        "    cluster_labels = kmeans.fit_predict(flattened_normalized_embedding_array)\n",
        "    sil_score = silhouette_score(flattened_normalized_embedding_array, cluster_labels)\n",
        "    sil_scores.append(sil_score)\n",
        "    print(f\"Number of clusters: {n_clusters}, Silhouette Score: {sil_score}\")\n",
        "\n",
        "plt.figure()\n",
        "plt.plot(range(2, 50), sil_scores)\n",
        "plt.title('Silhouette Score')\n",
        "plt.xlabel('Number of clusters')\n",
        "plt.ylabel('Silhouette Score')\n",
        "\n",
        "# Save the plot as a PNG image with 600 DPI\n",
        "plt.savefig('silhouette_analysis.png', dpi=600)\n",
        "\n",
        "# Display the plot\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "AUM8TYeDGvVl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Re-analyze adverb embeddings using optimized number of K-means clusters."
      ],
      "metadata": {
        "id": "KD4kR7JyF8gq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Reduce dimensionality using PCA\n",
        "num_dimensions = 2\n",
        "pca = PCA(n_components=num_dimensions)\n",
        "reduced_embedding_array = pca.fit_transform(flattened_normalized_embedding_array)\n",
        "\n",
        "# Use K-Means clustering\n",
        "num_clusters = 4\n",
        "kmeans = KMeans(n_clusters=num_clusters, random_state=0)\n",
        "cluster_labels = kmeans.fit_predict(reduced_embedding_array)\n",
        "\n",
        "# Create a dictionary to map adverbs to their true labels\n",
        "adverb_label_dict = {adverb: true_label for adverb, true_label in zip(adverbs, true_labels)}\n",
        "\n",
        "# Create a dictionary to map adverbs to their cluster labels\n",
        "adverb_cluster_dict = {adverb: cluster_label for adverb, cluster_label in zip(adverbs, cluster_labels)}\n",
        "\n",
        "# Create a dictionary that links adverbs with true labels to their cluster labels\n",
        "adverb_true_label_to_cluster = {adverb: {\"TrueLabel\": adverb_label_dict[adverb], \"ClusterLabel\": adverb_cluster_dict[adverb]} for adverb in adverbs}\n",
        "\n",
        "import csv\n",
        "\n",
        "# Specify the CSV file path\n",
        "output_csv_file_path = \"post_analysis.csv\"\n",
        "\n",
        "# Save the adverb_true_label_to_cluster dictionary as a CSV file\n",
        "with open(output_csv_file_path, mode='w', newline='', encoding='utf-8') as csv_file:\n",
        "    fieldnames = ['Adverb', 'TrueLabel', 'ClusterLabel']\n",
        "    writer = csv.DictWriter(csv_file, fieldnames=fieldnames)\n",
        "\n",
        "    writer.writeheader()\n",
        "    for adverb, labels in adverb_true_label_to_cluster.items():\n",
        "        writer.writerow({'Adverb': adverb, 'TrueLabel': labels['TrueLabel'], 'ClusterLabel': labels['ClusterLabel']})\n",
        "\n",
        "print(f\"CSV file '{output_csv_file_path}' has been created.\")\n",
        "\n",
        "\n",
        "# Generate a scatter plot with cluster centroids\n",
        "plt.figure(figsize=(10, 8))\n",
        "cluster_names = ['Cluster A', 'Cluster B', 'Cluster C', 'Cluster D']\n",
        "for i, cluster_name in enumerate(cluster_names):\n",
        "    plt.scatter(\n",
        "        reduced_embedding_array[cluster_labels == i, 0],\n",
        "        reduced_embedding_array[cluster_labels == i, 1],\n",
        "        label=cluster_name\n",
        "    )\n",
        "plt.scatter(\n",
        "    kmeans.cluster_centers_[:, 0],\n",
        "    kmeans.cluster_centers_[:, 1],\n",
        "    marker='X',\n",
        "    color='black',\n",
        "    label='Centroids'\n",
        ")\n",
        "\n",
        "# Label the points with adverbs and true labels\n",
        "for i, (adverb, true_label) in enumerate(zip(adverbs, true_labels)):\n",
        "    plt.annotate(f'{adverb} ({true_label})', (reduced_embedding_array[i, 0], reduced_embedding_array[i, 1]))\n",
        "\n",
        "plt.title('Cluster Plot with Centroids')\n",
        "plt.xlabel('Principal Component 1')\n",
        "plt.ylabel('Principal Component 2')\n",
        "plt.legend()\n",
        "\n",
        "# Save the plot as a PNG image with 600 DPI\n",
        "plt.savefig('post_analysis_scatterplot.png', dpi=600)\n",
        "\n",
        "# Display the plot\n",
        "plt.show()\n",
        "\n",
        "# Calculate the distance matrix between cluster centroids\n",
        "centroid_distances = cdist(kmeans.cluster_centers_, kmeans.cluster_centers_)\n",
        "\n",
        "# Print the distance matrix\n",
        "print(\"Distance Matrix of Centroids:\")\n",
        "print(centroid_distances)\n",
        "\n",
        "# Save the distance matrix to a CSV file\n",
        "np.savetxt(\"post_dist.csv\", centroid_distances, delimiter=\",\")"
      ],
      "metadata": {
        "id": "I_8tTylhGzSQ"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}