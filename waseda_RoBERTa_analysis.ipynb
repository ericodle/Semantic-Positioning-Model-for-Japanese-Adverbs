{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMgoh19OvKthGX1Jn1m7yGC",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ericodle/Semantic-Positioning-Model-for-Japanese-Adverbs/blob/main/waseda_RoBERTa_analysis.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Install required packages"
      ],
      "metadata": {
        "id": "8XuVgoQjYCnr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers\n",
        "!pip install sentencepiece\n",
        "!pip install fugashi\n",
        "!pip install unidic_lite\n",
        "!pip install japanize-matplotlib"
      ],
      "metadata": {
        "id": "zpo15bhQHW2d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Import script dependencies"
      ],
      "metadata": {
        "id": "XzQaqShHYB08"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer, AutoModelForMaskedLM\n",
        "import torch\n",
        "import numpy as np\n",
        "from sklearn.cluster import KMeans\n",
        "from sklearn.decomposition import PCA\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import japanize_matplotlib\n",
        "from sklearn.metrics import silhouette_score\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from scipy.spatial.distance import cdist"
      ],
      "metadata": {
        "id": "HDM5V1gtYTsG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Load model. Replace \"roberta-large-japanese\" with \"roberta-base-japanese\" as necessary."
      ],
      "metadata": {
        "id": "_P6619FxHgVq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = AutoTokenizer.from_pretrained(\"nlp-waseda/roberta-large-japanese\")\n",
        "model = AutoModelForMaskedLM.from_pretrained(\"nlp-waseda/roberta-large-japanese\")"
      ],
      "metadata": {
        "id": "Tc5ly3GRHpI4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Load csv list of adverbs and embed."
      ],
      "metadata": {
        "id": "S95mRbR7YBH2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the CSV file with adverbs\n",
        "csv_file_path = \"/content/unique_adverbs_column.csv\"\n",
        "\n",
        "# Read the CSV file using pandas\n",
        "data = pd.read_csv(csv_file_path, header=None, encoding=\"utf-8\")\n",
        "\n",
        "# Get the adverbs from the first column\n",
        "adverbs = data.iloc[:, 0].tolist()\n",
        "true_labels = data.iloc[:, 1].tolist()\n",
        "\n",
        "# Calculate embeddings and store them in a list\n",
        "embeddings = []\n",
        "for adverb in adverbs:\n",
        "    inputs = tokenizer.encode_plus(adverb, return_tensors=\"pt\", add_special_tokens=True)\n",
        "    with torch.no_grad():\n",
        "        output = model(**inputs)\n",
        "        hidden_states = output.logits  # Get the logits, which are equivalent to hidden states\n",
        "\n",
        "    # You can choose to take the mean or use the last hidden state\n",
        "    # Here, I'm using the mean of the hidden states\n",
        "    embedding = hidden_states.mean(dim=1).cpu().numpy()\n",
        "\n",
        "    embeddings.append(embedding)\n",
        "\n",
        "# Convert embeddings list to a numpy array\n",
        "embedding_array = np.array(embeddings)\n",
        "\n",
        "# Reshape the embedding array for PCA\n",
        "num_adverbs, num_tokens, embedding_dim = embedding_array.shape\n",
        "\n",
        "# Flatten the normalized embedding array before applying Min-Max Normalization\n",
        "flattened_embedding_array = embedding_array.reshape(-1, embedding_dim)"
      ],
      "metadata": {
        "id": "FkNipJRZYAdo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Normalize the embeding values"
      ],
      "metadata": {
        "id": "TUsEQ9pFYO0o"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Apply Min-Max Normalization\n",
        "min_max_scaler = MinMaxScaler()\n",
        "min_max_normalized_embedding_array = min_max_scaler.fit_transform(flattened_embedding_array)\n",
        "\n",
        "# Reshape the normalized embedding array back to its original shape\n",
        "min_max_normalized_embedding_array = min_max_normalized_embedding_array.reshape(num_adverbs, num_tokens, embedding_dim)\n",
        "\n",
        "# Flatten the 3D array to a 2D array\n",
        "flattened_normalized_embedding_array = min_max_normalized_embedding_array.reshape(-1, embedding_dim)"
      ],
      "metadata": {
        "id": "3UsGyobOYGLC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Reduce dimensionality by PCA, then cluster by K-means."
      ],
      "metadata": {
        "id": "Ip1gMjWTYPN6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Reduce dimensionality using PCA\n",
        "num_dimensions = 3  # Using 3 components for PCA\n",
        "pca = PCA(n_components=num_dimensions)\n",
        "reduced_embedding_array = pca.fit_transform(flattened_normalized_embedding_array)\n",
        "\n",
        "# Use K-Means clustering with 3 clusters\n",
        "num_clusters = 3\n",
        "kmeans = KMeans(n_clusters=num_clusters, random_state=0)\n",
        "cluster_labels = kmeans.fit_predict(reduced_embedding_array)\n",
        "\n",
        "# Generate a 3D scatter plot with cluster centroids\n",
        "fig = plt.figure(figsize=(10, 8))\n",
        "ax = fig.add_subplot(111, projection='3d')\n",
        "cluster_names = ['Cluster A', 'Cluster B', 'Cluster C']\n",
        "for i, cluster_name in enumerate(cluster_names):\n",
        "    ax.scatter(\n",
        "        reduced_embedding_array[cluster_labels == i, 0],\n",
        "        reduced_embedding_array[cluster_labels == i, 1],\n",
        "        reduced_embedding_array[cluster_labels == i, 2],  # Use the third component\n",
        "        label=cluster_name\n",
        "    )\n",
        "ax.scatter(\n",
        "    kmeans.cluster_centers_[:, 0],\n",
        "    kmeans.cluster_centers_[:, 1],\n",
        "    kmeans.cluster_centers_[:, 2],  # Use the third component\n",
        "    marker='X',\n",
        "    color='black',\n",
        "    label='Centroids'\n",
        ")\n",
        "\n",
        "# Label the points with adverbs and true labels\n",
        "for i, (adverb, true_label) in enumerate(zip(adverbs, true_labels)):\n",
        "    ax.text(\n",
        "        reduced_embedding_array[i, 0],\n",
        "        reduced_embedding_array[i, 1],\n",
        "        reduced_embedding_array[i, 2],  # Use the third component\n",
        "        f'{adverb} ({true_label})',\n",
        "        fontsize=8,\n",
        "        ha='center'\n",
        "    )\n",
        "\n",
        "ax.set_title('3D Cluster Plot with Centroids')\n",
        "ax.set_xlabel('Principal Component 1')\n",
        "ax.set_ylabel('Principal Component 2')\n",
        "ax.set_zlabel('Principal Component 3')\n",
        "ax.legend()\n",
        "\n",
        "# Save the plot as a PNG image with 600 DPI\n",
        "plt.savefig('pre_analysis_3d_scatterplot.png', dpi=600)\n",
        "\n",
        "# Display the plot\n",
        "plt.show()\n",
        "\n",
        "# Calculate the distance matrix between cluster centroids\n",
        "centroid_distances = cdist(kmeans.cluster_centers_, kmeans.cluster_centers_)\n",
        "\n",
        "# Print the distance matrix\n",
        "print(\"Distance Matrix of Centroids:\")\n",
        "print(centroid_distances)\n",
        "\n",
        "# Save the distance matrix to a CSV file\n",
        "np.savetxt(\"pre_dist.csv\", centroid_distances, delimiter=\",\")"
      ],
      "metadata": {
        "id": "YyUiNpuTIDbT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Perform Silhouette analysis"
      ],
      "metadata": {
        "id": "QYos6mLyYQRN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Silhouette Score\n",
        "sil_scores = []\n",
        "for n_clusters in range(2, 50):\n",
        "    kmeans = KMeans(n_clusters=n_clusters, random_state=0)\n",
        "    cluster_labels = kmeans.fit_predict(flattened_normalized_embedding_array)\n",
        "    sil_score = silhouette_score(flattened_normalized_embedding_array, cluster_labels)\n",
        "    sil_scores.append(sil_score)\n",
        "    print(f\"Number of clusters: {n_clusters}, Silhouette Score: {sil_score}\")\n",
        "\n",
        "plt.figure()\n",
        "plt.plot(range(2, 50), sil_scores)\n",
        "plt.title('Silhouette Score')\n",
        "plt.xlabel('Number of clusters')\n",
        "plt.ylabel('Silhouette Score')\n",
        "\n",
        "# Save the plot as a PNG image with 600 DPI\n",
        "plt.savefig('silhouette_analysis.png', dpi=600)\n",
        "\n",
        "# Display the plot\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "b6RB6CEAII1o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Re-analyze adverb embeddings using optimized number of K-means clusters."
      ],
      "metadata": {
        "id": "jPej-XjaYzh9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Reduce dimensionality using PCA\n",
        "num_dimensions = 2\n",
        "pca = PCA(n_components=num_dimensions)\n",
        "reduced_embedding_array = pca.fit_transform(flattened_normalized_embedding_array)\n",
        "\n",
        "# Use K-Means clustering\n",
        "num_clusters = 4\n",
        "kmeans = KMeans(n_clusters=num_clusters, random_state=0)\n",
        "cluster_labels = kmeans.fit_predict(reduced_embedding_array)\n",
        "\n",
        "# Create a dictionary to map adverbs to their true labels\n",
        "adverb_label_dict = {adverb: true_label for adverb, true_label in zip(adverbs, true_labels)}\n",
        "\n",
        "# Create a dictionary to map adverbs to their cluster labels\n",
        "adverb_cluster_dict = {adverb: cluster_label for adverb, cluster_label in zip(adverbs, cluster_labels)}\n",
        "\n",
        "# Create a dictionary that links adverbs with true labels to their cluster labels\n",
        "adverb_true_label_to_cluster = {adverb: {\"TrueLabel\": adverb_label_dict[adverb], \"ClusterLabel\": adverb_cluster_dict[adverb]} for adverb in adverbs}\n",
        "\n",
        "import csv\n",
        "\n",
        "# Specify the CSV file path\n",
        "output_csv_file_path = \"post_analysis.csv\"\n",
        "\n",
        "# Save the adverb_true_label_to_cluster dictionary as a CSV file\n",
        "with open(output_csv_file_path, mode='w', newline='', encoding='utf-8') as csv_file:\n",
        "    fieldnames = ['Adverb', 'TrueLabel', 'ClusterLabel']\n",
        "    writer = csv.DictWriter(csv_file, fieldnames=fieldnames)\n",
        "\n",
        "    writer.writeheader()\n",
        "    for adverb, labels in adverb_true_label_to_cluster.items():\n",
        "        writer.writerow({'Adverb': adverb, 'TrueLabel': labels['TrueLabel'], 'ClusterLabel': labels['ClusterLabel']})\n",
        "\n",
        "print(f\"CSV file '{output_csv_file_path}' has been created.\")\n",
        "\n",
        "# Generate a scatter plot with cluster centroids\n",
        "plt.figure(figsize=(10, 8))\n",
        "cluster_names = ['Cluster A', 'Cluster B', 'Cluster C', 'Cluster D']\n",
        "for i, cluster_name in enumerate(cluster_names):\n",
        "    plt.scatter(\n",
        "        reduced_embedding_array[cluster_labels == i, 0],\n",
        "        reduced_embedding_array[cluster_labels == i, 1],\n",
        "        label=cluster_name\n",
        "    )\n",
        "plt.scatter(\n",
        "    kmeans.cluster_centers_[:, 0],\n",
        "    kmeans.cluster_centers_[:, 1],\n",
        "    marker='X',\n",
        "    color='black',\n",
        "    label='Centroids'\n",
        ")\n",
        "\n",
        "# Label the points with adverbs and true labels\n",
        "for i, (adverb, true_label) in enumerate(zip(adverbs, true_labels)):\n",
        "    plt.annotate(f'{adverb} ({true_label})', (reduced_embedding_array[i, 0], reduced_embedding_array[i, 1]))\n",
        "\n",
        "plt.title('Cluster Plot with Centroids')\n",
        "plt.xlabel('Principal Component 1')\n",
        "plt.ylabel('Principal Component 2')\n",
        "plt.legend()\n",
        "\n",
        "# Save the plot as a PNG image with 600 DPI\n",
        "plt.savefig('post_analysis_scatterplot.png', dpi=600)\n",
        "\n",
        "# Display the plot\n",
        "plt.show()\n",
        "\n",
        "# Calculate the distance matrix between cluster centroids\n",
        "centroid_distances = cdist(kmeans.cluster_centers_, kmeans.cluster_centers_)\n",
        "\n",
        "# Print the distance matrix\n",
        "print(\"Distance Matrix of Centroids:\")\n",
        "print(centroid_distances)\n",
        "\n",
        "# Save the distance matrix to a CSV file\n",
        "np.savetxt(\"post_dist.csv\", centroid_distances, delimiter=\",\")"
      ],
      "metadata": {
        "id": "7jTHIdxrIPTr"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}